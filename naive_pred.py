# -*- coding: utf-8 -*-
"""access_notebook.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1i6kOJf_4cQTXUAlC1QXwYWzXesX8JWVa

imports
"""

import os
from sklearn.metrics import confusion_matrix
import pickle
import sim
import matplotlib.pyplot as plt
from sklearn.manifold import MDS
from sklearn.cluster import KMeans
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
import random
import seaborn as sns
import numpy as np
from collections import Counter
from transformers import AutoFeatureExtractor, Wav2Vec2ForPreTraining
from cp_feature_extractor import model_output
from sklearn.naive_bayes import MultinomialNB
import math
import pandas as pd
def load_ph_code_entry_map(
    path_phn_file: str#path of file
):  # loading phoneme to codeentry mapping for the file
    with open(path_phn_file, "rb") as f:
        phn_dict = pickle.load(f)
    return phn_dict


# def load_ph_code_entry_map_folder(
#     pickle_path#path of folder
# ):  # loading phoneme to codeentry mapping for the whole folder
#     phn_dict = {}
#     for subdir, dirs, files in os.walk(pickle_path):
#         for index,file in enumerate(files):
#             if index <= 5000 :
#                 if ".pkl" in file:
#                     phn_dict_f = load_ph_code_entry_map(
#                         subdir + "/" + file
#                     )  # loading phoneme to codeentry mapping for the file
#                     for p, val in phn_dict_f.items():
#                         phn_dict_tmp = dict(Counter(val))
#                         if p in phn_dict:
#                             for dis,count in phn_dict_tmp.items():
#                                 if dis in phn_dict[p]:
#                                     phn_dict[p][dis]+=count
#                                 else:
#                                     phn_dict[p][dis]=count
#                         else:
#                             phn_dict[p] = phn_dict_tmp
#     return phn_dict


def load_ph_code_entry_map_folder(
    pickle_path,dataset_path
):  # loading phoneme to codeentry mapping for the whole folder
    phn_dict = {}
    number_samples = 0
    for subdir, dirs, files in os.walk(dataset_path):
        for file in files:
            if file == "train.csv":
                #reading the csv file
                df = pd.read_csv(subdir + "/" + file)
                #read the first column of the csv file
                df = df.iloc[:, 0].values
                file_names = []
                for filename in df[:1000]:
                    file_names.append(filename.split('.')[0])
    for subdir, dirs, files in os.walk(pickle_path):
        for index,file in enumerate(files):
            if ".pkl" in file and file.split('.')[0] in file_names:
                phn_dict_f = load_ph_code_entry_map(
                    subdir + "/" + file
                )  # loading phoneme to codeentry mapping for the file
                for p, val in phn_dict_f.items():
                    phn_dict_tmp = dict(Counter(val))
                    if p in phn_dict:
                        for dis,count in phn_dict_tmp.items():
                            if dis in phn_dict[p]:
                                phn_dict[p][dis]+=count
                            else:
                                phn_dict[p][dis]=count
                    else:
                        phn_dict[p] = phn_dict_tmp
                number_samples+=1
    return phn_dict,number_samples

def write_discrete_units_prob(phn_to_dist_1, abs_discount,lang,folder_name):
    
    set_of_val= []
    for i,val in phn_to_dist_1.items():
        set_of_val = set_of_val+list(val.keys())
    set_of_val = list(set(set_of_val))
    # breakpoint()
    print(f"codebook enries:{sorted(set_of_val)}")
    print(f"number of codebook entries utilized out of 640:{len(set_of_val)}")

    discrete_count = {}
    for i,val in phn_to_dist_1.items():
        for dis,count in val.items():
            if dis in discrete_count:
                discrete_count[dis]+=count
            else:
                discrete_count[dis]=count
    # discrete_count_lst = []
    # for k,v in discrete_count.items():
    #     discrete_count_lst.extend([k]*v) 
    # return discrete_count_lst
    sum_count = sum(discrete_count.values())
    for val in set_of_val:
        if val in discrete_count:
            discrete_count[val] =float(discrete_count[val])/sum_count
        else:
            discrete_count[val] = 0
    discrete_count.update(zip(discrete_count, sim.absolute_discounting([i for i in discrete_count.values()], abs_discount, set_of_val)))
    #storing the discrete count dictionary in a pickle file using lang and folder name
    with open(f"discrete_count_{lang}_{folder_name}.pkl", "wb") as f:
        pickle.dump(discrete_count, f)
        
 
"""providing path folder which contains the phoneme to code entry mapping"""
def write_prb(lang,folder_name,dataset_path):

    #CP_wav2vec2_pkl
    abs_discount = 0.00000000002 #absolute discounting hyperparameter
    sample_num = {}
    for l in lang:
        codebook = 1
        print(f"loading {folder_name} codebook {codebook} for {l}")
        
        path_folder = (
            f"/corpora/common_phone_analysis/{folder_name}/codebook_{codebook}/{l}"
        )
        phn_dict1,number_samples = load_ph_code_entry_map_folder(path_folder,dataset_path+f"/{l}")
        
        codebook = 2
        path_folder = (
            f"/corpora/common_phone_analysis/{folder_name}/codebook_{codebook}/{l}"
        )
        phn_dict2,number_samples = load_ph_code_entry_map_folder(path_folder,dataset_path+f"/{l}")
        for key_1,val_1 in phn_dict2.items():
                for dis in val_1:
                    phn_dict1[key_1][dis+320] = phn_dict2[key_1][dis]
        phn_dict_final_wav2vec2=phn_dict1

        """calculating similarity with sorted phonemes and creating timit to ipa mappings"""

        write_discrete_units_prob(phn_dict1,abs_discount,l,folder_name)
        sample_num[l] = number_samples
    #write the sample number dictionary in a pickle file
    with open(f"sample_num_{folder_name}.pkl", "wb") as f:
        pickle.dump(sample_num, f)
    # model = MultinomialNB(alpha=1)
    # breakpoint()
    # train = np.array(train).reshape(-1,1).tolist()
    # model.fit(train, lang)
    # print("end")
def lang_pred(mdl_out,lang,folder_name):
    prb_class = {}
    for l in lang:
        #loading the discrete count dictionary
        prb = 0
        with open(f"discrete_count_{l}_{folder_name}.pkl", "rb") as f:
            discrete_count = pickle.load(f)
        for discrete_out in mdl_out:
            prb += math.log(discrete_count[discrete_out])
        #reading the sample number dictionary
        with open(f"sample_num_{folder_name}.pkl", "rb") as f:
            sample_num = pickle.load(f)
        prb_class[l] = prb+math.log(sample_num[l]/sum(sample_num.values()))
    #key of maximum value of prb_class is the predicted language
    prb_class_val = {}
    prb_class_val_ret = {}
    #prb_class values to probability to store in prb_class_val without using math.exp
    for k,v in prb_class.items():
        prb_class_val[k] = v-max(prb_class.values())
    for key,val in prb_class_val.items():
        prb_class_val_ret[key] = math.exp(val)/sum([math.exp(i) for i in prb_class_val.values()])
    return max(prb_class, key=prb_class.get),prb_class_val_ret

def main(lang,folder_name, dataset_path): 
    feature_extractor = AutoFeatureExtractor.from_pretrained("facebook/wav2vec2-large-xlsr-53")
    model = Wav2Vec2ForPreTraining.from_pretrained("facebook/wav2vec2-large-xlsr-53")
    accuracy = 0
    num_files = 0
    pred_languages = []
    actual_languages= []
    cf_prb_m = {}
    for subdir, dirs, files in os.walk(dataset_path):
        for file in files:
            if file == "test.csv":
                #reading the csv file
                df = pd.read_csv(subdir + "/" + file)
                #read the first column of the csv file
                df = df.iloc[:, 0].values
                for filename in df[:1000]:
                    audio_path = subdir + "/wav/" + filename.split('.')[0]+".wav"
                    codebook = 1
                    mdl_out_cd_1 = model_output(audio_path = audio_path,codebook = codebook,feature_extractor = feature_extractor,model=model)
                    codebook = 2
                    mdl_out_cd_2 = model_output(audio_path = audio_path,codebook = codebook,feature_extractor = feature_extractor,model=model)
                    mdl_out = mdl_out_cd_1+np.array(np.array(mdl_out_cd_2)+320).tolist()
                    pred_lang,prb_class_val_ret = lang_pred(mdl_out,lang,folder_name)
                    
                    #adding dictionaries prb_class_val_ret and cf_prb_m
                    cf_prb_m = {k: cf_prb_m.get(k, 0) + prb_class_val_ret.get(k, 0) for k in set(cf_prb_m) | set(prb_class_val_ret)}
                    pred_languages.append(pred_lang)
                    actual_languages.append(subdir.split('/')[-1])
                    if pred_lang == subdir.split('/')[-1]:
                        accuracy+=1
                    else:
                        # print(f"actual:{subdir.split('/')[-1]} predicted:{pred_lang}")
                        pass
                    num_files+=1
    #divide the values of cf_prb_m by the number of files
    cf_prb_m = {k: v/num_files for k, v in cf_prb_m.items()}
    
    print(f"accuracy using model {folder_name} is {accuracy/num_files}")
    return accuracy/num_files,pred_languages,actual_languages,cf_prb_m
if  __name__ == "__main__":
    # write_prb(lang=["de","en","es","fr","it","ru"],folder_name= "CP_wav2vec2_pkl",dataset_path = "/corpora/common_phone")
    folder_name = "CP_xlsr_pkl"
    lang = ["de","en","es","fr","it","ru"]
    pred_language = []
    actual_language = [] 
    cf_prbs_m = {}
    for l in lang:
        print("accuracy for language",l)
        _,pred_language_r,actual_language_r,cf_prb_m = main(lang = ["de","en","es","fr","it","ru"],folder_name = folder_name, dataset_path = f"/corpora/common_phone/{l}")
        cf_prbs_m[l] = cf_prb_m
        pred_language+=pred_language_r
        actual_language+=actual_language_r
    #accuracy from pred_language and actual_language
    accuracy = 0
    for i in range(len(pred_language)):
        if pred_language[i] == actual_language[i]:
            accuracy+=1
    print(f"accuracy using model {folder_name} is {accuracy/len(pred_language)}")
    cf_m = confusion_matrix(actual_language, pred_language)
    #plot confusion matrix cf_m with labels as lang list on x and y axis
    plt.figure(figsize=(10,10))
    sns.heatmap(cf_m, annot=True, fmt="d",xticklabels=lang, yticklabels=lang)
    plt.title("Confusion matrix")
    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    plt.show()
    #save the plot as png
    plt.savefig(f"confusion_matrix_{folder_name}.png")
    #plot cf_prbs_m as heatmap
    plt.figure(figsize=(10,10))
    #rearrange values of dictionary cf_prbs_m based on a list lang
    for k,v in cf_prbs_m.items(): 
        cf_prbs_m[k] =  {key:cf_prbs_m[k][key] for key in sorted(cf_prbs_m[k])}
    sns.heatmap(pd.DataFrame(cf_prbs_m), annot=True, fmt="f",xticklabels=lang, yticklabels=lang)
    plt.title("Confusion matrix probability")
    plt.ylabel('True label') 
    plt.xlabel('Predicted label')
    plt.show()
    #save the plot as png
    plt.savefig(f"confusion_matrix_prob_{folder_name}.png")
