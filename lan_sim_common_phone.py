# -*- coding: utf-8 -*-
"""access_notebook.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1i6kOJf_4cQTXUAlC1QXwYWzXesX8JWVa

imports
"""

import os
import pickle
import sim
import matplotlib.pyplot as plt
from sklearn.manifold import MDS
from sklearn.cluster import KMeans
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
import random
import seaborn as sns
import numpy as np
from collections import Counter

"""functions to load the saved file which contains mapping from phoneme to codebook entry"""
def get_sim_mat2(sm):
    return 1 / (np.linalg.norm(sm[:, np.newaxis] - sm[np.newaxis], axis=-1) + 1/len(sm))
def argsort_sim_mat(sm):
    idx = [np.argmax(np.sum(sm, axis=1))]  # a
    for i in range(1, len(sm)):
        sm_i = sm[idx[-1]].copy()
        sm_i[idx] = -1
        idx.append(np.argmax(sm_i))  # b
    return np.array(idx)

def load_ph_code_entry_map(
    path_phn_file: str#path of file
):  # loading phoneme to codeentry mapping for the file
    with open(path_phn_file, "rb") as f:
        phn_dict = pickle.load(f)
    return phn_dict


def load_ph_code_entry_map_folder(
    pickle_path#path of folder
):  # loading phoneme to codeentry mapping for the whole folder
    phn_dict = {}
    for subdir, dirs, files in os.walk(pickle_path):
        for index,file in enumerate(files):
            if index <= 500 :
                if ".pkl" in file and file.startswith('common'):
                    phn_dict_f = load_ph_code_entry_map(
                        subdir + "/" + file
                    )  # loading phoneme to codeentry mapping for the file
                    for p, val in phn_dict_f.items():
                        phn_dict_tmp = dict(Counter(val))
                        if p in phn_dict:
                            for dis,count in phn_dict_tmp.items():
                                if dis in phn_dict[p]:
                                    phn_dict[p][dis]+=count
                                else:
                                    phn_dict[p][dis]=count
                        else:
                            phn_dict[p] = phn_dict_tmp
    return phn_dict
def plot_compress(lang,folder_name,sim_mt, phn_to_dist_1_keys,type_of_compression = "TSNE", kmeans = False):
    if type_of_compression == "mds":
        compress = MDS(n_components=2, dissimilarity="precomputed", n_jobs=-1).fit_transform(
            sim_mt
        )
    elif type_of_compression == "TSNE":
        # compress = TSNE(n_components=2, learning_rate='auto',init='random', perplexity=3,random_state = 0).fit_transform(sim_mt)
        compress = TSNE(n_components=2,verbose=1, perplexity=9, n_iter=1000, learning_rate=200,random_state = 0).fit_transform(sim_mt)
    elif type_of_compression == "PCA":
        compress = PCA(n_components=2,random_state = 0).fit_transform(sim_mt)  
        # val1 = compress[:,0]
        # val2 = compress[:,1]
        # compress[:,1] = np.array(val1)*-1
        # compress[:,0] = np.array(val2)*-1
    if kmeans == True:
        kmns = KMeans(n_clusters=12, random_state=0).fit(compress)
        colors_cluster = kmns.labels_
        sns.scatterplot(x = np.array(compress[:, 0]), y = np.array(compress[:, 1]),c = colors_cluster)
        
    else:
        sns.scatterplot(compress[:, 0], compress[:, 1])
    for i, phn in enumerate(phn_to_dist_1_keys):
        v = random.uniform(0, 0.5)
        plt.annotate(phn, (compress[:, 0][i], compress[:, 1][i]))
    plt.xlabel("{type_of_compression} axis 1")
    plt.ylabel("{type_of_compression} axis 2")
    plt.savefig(f"compress_{lang}_{folder_name}.pdf", bbox_inches="tight")

def lst_of_discrete_units(lang = "en",folder_name = "CP_xlsr_pkl"):
    codebook = 1

    print(f"loading {folder_name} codebook {codebook} for {lang}")
    path_folder = (
        f"/corpora/common_phone_analysis/{folder_name}/codebook_{codebook}/{lang}"
    )
    phn_dict1 = load_ph_code_entry_map_folder(path_folder)

    codebook = 2
    path_folder = (
        f"/corpora/common_phone_analysis/{folder_name}/codebook_{codebook}/{lang}"
    )
    phn_dict2 = load_ph_code_entry_map_folder(path_folder)
    for key_1,val_1 in phn_dict2.items():
            for dis in val_1:
                phn_dict1[key_1][dis+320] = phn_dict2[key_1][dis]
    phn_dict_final_wav2vec2=phn_dict1
    #traverse through 640 times in phn_dict_final_wav2vec2 dictionary create number of times each discrete_unit is present in the dictionary
    dicrete_count = [0]*640
    for discrete_index in range(640):
        for phn, dist in phn_dict_final_wav2vec2.items():
            if discrete_index in dist:
                dicrete_count[discrete_index]+=dist[discrete_index]
    #divice the dicrete_count by the total sum of the dicrete_count
    dicrete_count = np.array(dicrete_count)/sum(dicrete_count)
    abs_discount = 0.00000000002 
    dicrete_count = sim.absolute_discounting(dicrete_count, abs_discount, dicrete_count)
    return dicrete_count
if __name__ == "__main__":
    languages = ["es","fr","de","ru","it","en"]
    discrete_units_lang_dict = {}
    folder_name = "CP_xlsr_pkl"
    for lang in languages:
        discrete_units_lang_dict[lang] = lst_of_discrete_units(lang,folder_name)
    sim_mt = np.zeros((len(languages), len(languages)))
    for num1, i in enumerate(languages):
        for num2, j in enumerate(languages):
            sim_mt[num1][num2] = sim.distance.jensenshannon(
                discrete_units_lang_dict[i], discrete_units_lang_dict[j]
            )
            print(f"sim between {i} and {j}:", sim_mt[num1][num2])
    sim_mt_2 = get_sim_mat2(sim_mt)
    idx = argsort_sim_mat(sim_mt_2)
    sim_mat_sorted = sim_mt[idx, :][:, idx]
    labels =  [languages[i] for i in idx]

    """plotting similarity matrix, dendogram and compression"""
    lang = ""
    sim.plot_sim(lang,folder_name,sim_mat_sorted, labels, labels,dend = True)
    plt.figure()
    plot_compress(lang,folder_name,sim_mat_sorted, labels,"TSNE",False)
    plt.show()
    
